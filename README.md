# building-gpt2-clone-124M-from-scratch
A custom implementation of OpenAI's GPT-2(124M) from scratch, following the paper "Language Models are Unsupervised Multitask Learners".We train the model on the FineWeb dataset -10B tokens (27.6GB).
